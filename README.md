# Optim
Visualization project for Gradient Descent, Momentum based Gradient Descent and Nesterov accelerated Gradient Descent 
From a nice-to-have skill to a must-have skill, Data Visualization has become an important part of business success.
Apart from adding value to the business and decision-making, Visualization can help with an intuitive understanding of many mathematical proofs like  gradients,
convergence to an optimal value, and many more. This video helps to visualize three basic and important Optimization Algorithms - 
Gradient Descent, Momentum based Gradient Descent and Nesterov Accelerated Gradient Descent.
I used a learning rate of 0.01 for all three Algorithms and a gamma value of 0.9 for MBGD and NAGD.
Here in this projects, you can see the pros and cons of all 3 algorithms on two different functions.
For a convex function, the vanilla-flavored Gradient Descent takes forever to get to the optimal value, whereas for the other function,
the traditional Gradient Descent although started slow it does reach optimal value even before MBGD and NAGD.
This is simply because MBGD and NAGD make greedy decisions to converge to Minima.
I have also made the contour plots for both the functions and projected the path from the 3D surface onto a 2D contour plot.
